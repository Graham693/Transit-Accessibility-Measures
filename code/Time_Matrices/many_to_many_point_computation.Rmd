---
title: "Many-to-Many Point Computation Script"
author: "Luka Vukovic"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

### Loading libraries
```{r include=FALSE}

# Main
library(r5r)
#if (Sys.getenv("JAVA_HOME")!="")
#  Sys.setenv(JAVA_HOME="")
library(rJava)
library(sf)
library(data.table)

# Visualization
library(ggplot2)
library(mapview); mapviewOptions(platform = 'leafgl')

# Convenience
library(tidyverse)
library(glue)
library(rlist)

```


### 1. Setup the Network Graph

- If the vancouver_canada.osm.pbf file needs to be converted to an .osm, one can use a binary osm converter available at: https://wiki.openstreetmap.org/wiki/Osmconvert#Binaries

```{r message=FALSE, warning=FALSE}
## Allocate 4G RAM to Java
options(java.parameters = "-Xmx6g")

## Build transport network, pointing to path where OSM and GTFS data are located
r5r_core <- setup_r5(data_path = getwd(), verbose = FALSE)
```


### 2. Load origin/destination points

```{r, warning=FALSE}

# Dissemination Blocks
origins <- fread(file.path("../../data/clean", "vancouver_db.csv"))
origins <- origins[, c(1,3,4)]
colnames(origins)[1] <- 'id'
origins$id <- as.character(origins$id)  # numeric to char

# Cultural/Art facilities
destinations <- fread(file.path("../../data/clean", "vancouver_facilities_2.csv"))
destinations <- destinations[, 1:3]

destinations$lat <-  as.numeric(destinations$lat) # char to numeric
destinations$lon <-  as.numeric(destinations$lon) # char to numeric
destinations$id <- as.character(destinations$id)  # numeric to char

destinations <- destinations[complete.cases(destinations)] # remove NA rows

# Peek
head(origins)
head(destinations)

# Check
c(nrow(origins), nrow(unique(origins[,1])))
c(nrow(destinations), nrow(unique(destinations[,1])))
```

```{r}
# Testing sample
sample_origins <- origins[sample(.N, 3000)]
sample_destinations <- destinations[sample(.N, 432)] # uses all destinations

# Peek
sample_origins
sample_destinations
```


### 3. Set constraints

```{r}
# Non-transit : WALK, BICYCLE, CAR, BICYCLE_RENT, CAR_PARK
# Transit: TRAM, SUBWAY, RAIL, BUS, FERRY, CABLE_CAR, GONDOLA, FUNICULAR
mode <- c('WALK', 'TRANSIT')

max_walk_dist <- 1000 # 1 km

max_trip_duration <- 120 # 2 hours

max_rides <- 3 # max transfers

```


### 4. Compute Expanded Travel Time Matrix

- We average transit times across all weekly transit schedules so we compute travel time on:
  - A weekday
  - Saturday
  - Sunday
- We also want average transit times across time of day so we compute travel times from:
  - 7am to 7pm at every hour mark with a departure window of 30 minutes.
  - A window of 1 equates to a 5 minute departure window so we will use 6.

```{r}
# default walk speed = 3.6 km/h

all_ttms <- list()

for (day in 14:16) {          # May 14=Fri, 15=Sat, 16=Sun
  for (time in 7:19) {        # 7 to 19 hours
    
    departure_datetime <- as.POSIXct(glue("{day}-05-2021 {time}:00:00"), format="%d-%m-%Y %H:%M:%S")
    
    ttm <- travel_time_matrix(r5r_core = r5r_core,
                          origins = origins,
                          destinations = destinations,
                          departure_datetime = departure_datetime,
                          time_window = 30,
                          
                          # constrains
                          mode = mode,
                          max_walk_dist = max_walk_dist,
                          max_trip_duration = max_trip_duration,
                          max_rides = max_rides,
                          verbose = FALSE)
    
    all_ttms <-  list.append(all_ttms, ttm) # very slow: rbind(all_ttm, ttm)
    
    print(glue('Progress: {round(((day-14)*12 + time-6)/37*100, 1)}%'))
  }
}

# Fast way to bind all data.frames
TTM <- rbindlist(all_ttms)

print('COMPLETED')

summary(TTM)
```


### 5. Aggregate Travel Time Matrix without any Destination Weights

```{r}
# Aggregate on each unique transit trip
# This computes the average travel time to the same destination (weekly variation in trips)
TTM_agg <- TTM %>%
           group_by(fromId, toId) %>% 
           summarise(
             avg_unique_time = mean(travel_time), 
             sd_unique_time = sd(travel_time)
           )

print('First aggregation:')
summary(TTM_agg)


# Fill NAs in standard deviation
# Rare trip have a sd equal to inf or NA since there may be only 1 sample
# We'll replace them with a reasonable guess using the 90th percentile variation
# since their variations will likely be on the larger side
upper90_sd <- quantile(TTM_agg$sd_unique_time, 0.9, na.rm=TRUE)
TTM_agg <- TTM_agg %>% replace_na(list('sd_unique_time'=upper90_sd))

print('First Aggregation with NAs replaced:')
summary(TTM_agg)


# Aggregate on each origin
# This computes the average travel time from each origin to all destinations (variation in all trip)
TTM_final <- TTM_agg %>%
               group_by(fromId) %>% 
               summarise(
                 avg_time_to_allpoi = mean(avg_unique_time), 
                 sd_time_to_allpoi = sd(avg_unique_time),
                 avg_sd_time_to_uniquepoi = mean(sd_unique_time),
                 n_accessible_poi = n()
               )

upper90_sd <- quantile(TTM_final$sd_time_to_allpoi, 0.9, na.rm=TRUE)
TTM_final <- TTM_final %>% replace_na(list('sd_time_to_allpoi'=upper90_sd))

print('Second aggregation:')
summary(TTM_final)


# Export
write.csv(TTM_final, "../../data/clean/ttm_data.csv", row.names = FALSE)

```

```{r}
# Check how many origins/destinations had trips
paste('Percent Origins considered:', round(length(unique(TTM_agg$fromId))/nrow(origins) *100, 2), '%')
paste('Percent Destinations considered:', round(length(unique(TTM_agg$toId))/nrow(destinations) *100, 2), '%')
```



### 6. Import the Destination (POI) Popularity Weights

```{r}
# Import


# Initialize destination popularity dataframe
POI_popularity <- data.frame('id'= sample_destinations$id, stringsAsFactors = TRUE)

# Generate POI popularity weights
POI_popularity$wt <- runif(nrow(sample_destinations))

POI_popularity
```



### 7. Reperform the Second Aggregation to Include the Destination Weights

```{r}
# Add popularity weight column using levels
TTM_agg_w_wts <- left_join(TTM_agg, POI_popularity, by = c('toId'='id'))

# Fill NAs (rare trip have no sd) with 90th percentile variation
upper90_sd <- quantile(TTM_agg_w_wts$sd_unique_time, 0.9, na.rm=TRUE)
TTM_agg_w_wts <- TTM_agg_w_wts %>% replace_na(list('sd_unique_time'=upper90_sd))

# Aggregate on each origin
# This computes the average travel time from each origin to all destinations (variation in all trip)
TTM_final_wts <- TTM_agg_w_wts %>%
               group_by(fromId) %>% 
               summarise(
                 avg_time_to_allpoi = mean(avg_unique_time), 
                 sd_time_to_allpoi = sd(avg_unique_time),
                 avg_sd_time_to_uniquepoi = mean(sd_unique_time),
                 n_accessible_poi = sum(wt)
               )

# Export
write.csv(TTM_final_wts, "../../data/clean/ttm_data_weighted.csv", row.names = FALSE)

summary(TTM_final_wts)

```

# Experimental Score Conversion

### 8. Convert Average Transit Time to [0-1] Score
```{r}
# Naive approach
# 1 is perfect transit accessibility /// 0 is no transit accessibility
# Higher avg_time = Lower score (inverse)
# Higher sd_time = Lower score (inverse)
# More accessible destinations = Higher score (multiply)
# Therefore: score = n_destinations /(avg_time*sd_time)


# Without Weights
TTM_final$score <- TTM_final$n_accessible_poi / (TTM_final$avg_time_to_allpoi*TTM_final$avg_sd_time_to_uniquepoi)
TTM_final <- TTM_final[order(TTM_final$score, decreasing=TRUE,na.last=FALSE), ]
# Peek at undefined (NA/inf) values
TTM_final


# Without Weights
TTM_final_wts$score <- TTM_final_wts$n_accessible_poi / (TTM_final_wts$avg_time_to_allpoi*TTM_final_wts$avg_sd_time_to_uniquepoi)
TTM_final_wts <- TTM_final_wts[order(TTM_final_wts$score, decreasing=TRUE,na.last=FALSE), ]
# Peek at undefined (NA/inf) values
TTM_final_wts

```



```{r}
# Cut off infinite values
TTM_final <- TTM_final[23:nrow(TTM_final),] # must manually input the cut off index +1
TTM_final_wts <- TTM_final_wts[23:nrow(TTM_final_wts),] # must manually input the cut off index +1

# Normalize
min_ <- min(TTM_final$score, na.rm=TRUE)
max_ <- max(TTM_final$score, na.rm=TRUE)
TTM_final$norm_score <- (TTM_final$score-min_)/(max_-min_)

min_ <- min(TTM_final_wts$score, na.rm=TRUE)
max_ <- max(TTM_final_wts$score, na.rm=TRUE)
TTM_final_wts$norm_score <- (TTM_final_wts$score-min_)/(max_-min_)

# Distribution
hist(TTM_final$norm_score)
hist(TTM_final_wts$norm_score)



```

```{r}
# Export
ttm_unweighted_scores <- TTM_final[,c(1,6,7)]
ttm_weighted_scores <- TTM_final_wts[,c(1,6,7)]
write.csv(TTM_final, "../../data/clean/ttm_unweighted_scores.csv", row.names = FALSE)
write.csv(TTM_final_wts, "../../data/clean/ttm_weighted_scores.csv", row.names = FALSE)

```



### 9. Visualization of Street Network
```{r}
# extract OSM network
#street_net <- street_network_to_sf(r5r_core)


# Generate a static View
#ggplot() +
#  geom_sf(data = street_net$edges, color='gray85') +
#  theme_void()


# Generate an interactive View
#mapview(street_net)
```


### 10. Visualization of Average Travel Times

```{r}

```



