---
title: "Many-to-Many Point Computation Script"
author: "Luka Vukovic"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

### Loading libraries
```{r include=FALSE}

# Main
library(r5r)
#if (Sys.getenv("JAVA_HOME")!="")
#  Sys.setenv(JAVA_HOME="")
library(rJava)
library(sf)
library(data.table)

# Visualization
library(ggplot2)
library(mapview); mapviewOptions(platform = 'leafgl')

# Convenience
library(tidyverse)
library(glue)
library(rlist)

```


### 1. Setup the Network Graph

- If the vancouver_canada.osm.pbf file needs to be converted to an .osm, one can use a binary osm converter available at: https://wiki.openstreetmap.org/wiki/Osmconvert#Binaries

```{r message=FALSE, warning=FALSE}
## Allocate 4G RAM to Java
options(java.parameters = "-Xmx6g")

## Build transport network, pointing to path where OSM and GTFS data are located
r5r_core <- setup_r5(data_path = getwd(), verbose = FALSE)
```


### 2. Load origin/destination points

```{r, warning=FALSE}

# Dissemination Blocks
origins <- fread(file.path("../../data/clean", "vancouver_db.csv"))
origins <- origins[, c(1,3,4)]
colnames(origins)[1] <- 'id'
origins$id <- as.character(origins$id)  # numeric to char

# Cultural/Art facilities
destinations <- fread(file.path("../../data/clean", "vancouver_facilities_2.csv"))
destinations <- destinations[, 1:3]

destinations$lat <-  as.numeric(destinations$lat) # char to numeric
destinations$lon <-  as.numeric(destinations$lon) # char to numeric
destinations$id <- as.character(destinations$id)  # numeric to char

destinations <- destinations[complete.cases(destinations)] # remove NA rows

# Peek
head(origins)
head(destinations)

# Check
c(nrow(origins), nrow(unique(origins[,1])))
c(nrow(destinations), nrow(unique(destinations[,1])))
```

```{r}
## Testing samples
#sample_origins <- origins[sample(.N, 3000)]
#sample_destinations <- destinations[sample(.N, 432)] # uses all destinations
#sample_origins
#sample_destinations
```


### 3. Set constraints

```{r}
# Non-transit : WALK, BICYCLE, CAR, BICYCLE_RENT, CAR_PARK
# Transit: TRAM, SUBWAY, RAIL, BUS, FERRY, CABLE_CAR, GONDOLA, FUNICULAR
mode <- c('WALK', 'TRANSIT')

max_walk_dist <- 1000 # 1 km

max_trip_duration <- 120 # 2 hours

max_rides <- 3 # max transfers

```


### 4. Compute Expanded Travel Time Matrix

- We average transit times across all weekly transit schedules so we compute travel time on:
  - A weekday
  - Saturday
  - Sunday
- We also want average transit times across time of day so we compute travel times from:
  - 7am to 7pm at every hour mark with a departure window of 30 minutes.
  - A window of 1 equates to a 5 minute departure window so we will use 6.

```{r}
# default walk speed = 3.6 km/h

all_ttms <- list()

for (day in 14:16) {          # May 14=Fri, 15=Sat, 16=Sun
  for (time in 7:19) {        # 7 to 19 hours
    
    departure_datetime <- as.POSIXct(glue("{day}-05-2021 {time}:00:00"), format="%d-%m-%Y %H:%M:%S")
    
    ttm <- travel_time_matrix(r5r_core = r5r_core,
                          origins = origins,
                          destinations = destinations,
                          departure_datetime = departure_datetime,
                          time_window = 30,
                          
                          # constrains
                          mode = mode,
                          max_walk_dist = max_walk_dist,
                          max_trip_duration = max_trip_duration,
                          max_rides = max_rides,
                          verbose = FALSE)
    
    all_ttms <-  list.append(all_ttms, ttm) # very slow: rbind(all_ttm, ttm)
    
    print(glue('Progress: {round(((day-14)*12 + time-6)/37*100, 1)}%'))
  }
}

# Fast way to bind all data.frames
TTM <- rbindlist(all_ttms)

print('COMPLETED')

summary(TTM)
```


### 5. Aggregate Travel Time Matrix without any Destination Weights

- In order to avoid NA or inf values in the score computation, we need to replace all standard deviations that are zero or NA with an appropriate value.
- A zero standard deviation can be replaced with the 1% quantile
- An NA standard deviation is due to there only being 1 sample in the computation. Since there's only a single possible trip we'll assume the variation in operation is higher than average, so the 80% quantile in variation will be used as replacement.

In the second aggregation we take the standard deviation of transit time to all points of interest from a given block. If the block can only access 1 point of interest, it's standard deviation will be NA. We can thus replace it with the 80% quantile assuming that its variation in trip time will be greater on average.

```{r}

## FIRST AGGREGATION
# aggregate on each unique transit trip computing the avg trip travel time to the same destination
#TTM_agg <- TTM %>%
#           group_by(fromId, toId) %>% 
#           summarise(
#             avg_unique_time = mean(travel_time), 
#             sd_unique_time = sd(travel_time)
#           )
print('First aggregation:'); summary(TTM_agg)


## FIX FIRST AGGREGATION

# fill NAs in standard deviation with 80th percentile value
upper80_sd <- quantile(TTM_agg$sd_unique_time, 0.8, na.rm=TRUE)
TTM_agg <- TTM_agg %>% replace_na(list('sd_unique_time'=upper80_sd))

# replace zero standard deviations to avoid infinity computations later on
lower01_sd <- quantile(TTM_agg$sd_unique_time, 0.01, na.rm=TRUE)
TTM_agg$sd_unique_time[(TTM_agg$sd_unique_time == 0)] <-  lower01_sd

print('First Aggregation with sd NAs+Zeros replaced:'); summary(TTM_agg)


## SECOND AGGREGATION
# aggregates on each origin computing avg travel time to all poi from each origin
TTM_final <- TTM_agg %>%
               group_by(fromId) %>% 
               summarise(
                 avg_time_to_allpoi = mean(avg_unique_time), 
                 sd_time_to_allpoi = sd(avg_unique_time),
                 avg_sd_time_to_uniquepoi = mean(sd_unique_time),
                 n_accessible_poi = n()
               )

# Fill NAs in standard deviation with 90th percentile value
upper80_sd <- quantile(TTM_final$sd_time_to_allpoi, 0.8, na.rm=TRUE)
TTM_final <- TTM_final %>% replace_na(list('sd_time_to_allpoi'=upper80_sd))

print('Second aggregation:'); summary(TTM_final)

```

```{r}
# Check how many origins/destinations had trips
paste('Percent Origins considered:', round(length(unique(TTM_agg$fromId))/nrow(origins) *100, 2), '%')
paste('Percent Destinations considered:', round(length(unique(TTM_agg$toId))/nrow(destinations) *100, 2), '%')

# Export for convenience
write.csv(TTM_final, "../../data/clean/ttm_data.csv", row.names = FALSE)

```



### 6. Import the Destination (POI) Popularity Weights

```{r}
# Import

# Initialize destination popularity dataframe
POI_popularity <- data.frame('id'= destinations$id, stringsAsFactors = TRUE)

# Generate POI popularity weights
# using rbeta density 
# assumes cultural facilities tend to be popular or unpopular, not inbetween
plot(density(rbeta(nrow(destinations), shape1=0.65, shape2=0.65)))
POI_popularity$wt <- rbeta(nrow(destinations), shape1=0.8, shape2=0.8)

POI_popularity


```



### 7. Reperform the Second Aggregation to Include the Destination Weights

```{r}
# Join dfs to have a popularity weight column
TTM_agg_w_wts <- left_join(TTM_agg, POI_popularity, by = c('toId'='id'))


# Aggregate on each origin
# This computes the average travel time from each origin to all destinations (variation in all trip)
TTM_final_wts <- TTM_agg_w_wts %>%
                 group_by(fromId) %>% 
                 summarise(
                   avg_time_to_allpoi = mean(avg_unique_time), 
                   sd_time_to_allpoi = sd(avg_unique_time),
                   avg_sd_time_to_uniquepoi = mean(sd_unique_time),
                   n_accessible_poi = sum(wt)
                 )

# Fill NAs (rare trip have no sd) with 80th percentile variation
upper80_sd <- quantile(TTM_final_wts$sd_time_to_allpoi, 0.8, na.rm=TRUE)
TTM_final_wts <- TTM_final_wts %>% replace_na(list('sd_time_to_allpoi'=upper80_sd))

summary(TTM_final_wts)

```

```{r}
## Normalize data

# [0-1] Normalization Function
normalize <- function(vector) {
  minimum <- min(vector, na.rm=TRUE)
  maximum <- max(vector, na.rm=TRUE)
  range <- maximum-minimum
  
  normed <- (vector-minimum*0.99)/range # use 0.99 to avoid zero values
  return(normed)
}

# Normalize data
TTM_final_normalized <- data.frame(cbind('fromId' = TTM_final$fromId, apply(TTM_final[2:5], 2, normalize)))
TTM_final_wts_normalized <- data.frame(cbind('fromId' = TTM_final_wts$fromId, apply(TTM_final_wts[2:5], 2, normalize)))
```


# Experimental Score Conversion

### 8. Convert Average Transit Time to [0-1] Score
```{r}
# Naive approach
# 1 is perfect transit accessibility /// 0 is no transit accessibility
# Higher avg_time = Lower score (inverse)
# Higher sd_time = Lower score (inverse)
# More accessible destinations = Higher score (multiply)
# Therefore: score = n_destinations /(avg_time*sd_time)

Naive_score_fxn <- function(n_accessible, mean_tt, sd_tt) {
  score <- n_accessible / (mean_tt*sd_tt)
  #score <- normalize(score) # incorporate normalization
  return(score)
}

# Score without Weights
TTM_final_normalized$score <- Naive_score_fxn(TTM_final$n_accessible_poi,
                                              TTM_final$avg_time_to_allpoi,
                                              TTM_final$avg_sd_time_to_uniquepoi)
# Score with Weights
TTM_final_wts_normalized$score <- Naive_score_fxn(TTM_final_wts$n_accessible_poi,
                                                  TTM_final_wts$avg_time_to_allpoi,
                                                  TTM_final_wts$avg_sd_time_to_uniquepoi)

# Sort scores
TTM_final_normalized <- TTM_final_normalized[order(TTM_final_normalized$score, decreasing=TRUE,na.last=FALSE), ]
TTM_final_wts_normalized <- TTM_final_wts_normalized[order(TTM_final_wts_normalized$score, decreasing=TRUE,na.last=FALSE), ]

# Peek
TTM_final_normalized
TTM_final_wts_normalized

```



```{r}
# fix weird value of 10
TTM_final_normalized$score[1] <- 4.76

# Normalize scores
TTM_final_normalized$score <- normalize(TTM_final_normalized$score)
TTM_final_wts_normalized$score <- normalize(TTM_final_wts_normalized$score)

# Distribution
plot(density(TTM_final_normalized$score))
plot(density(TTM_final_wts_normalized$score))

# Export
write.csv(TTM_final_normalized, "../../data/clean/ttm_unweighted_scores.csv", row.names = FALSE)
write.csv(TTM_final_wts_normalized, "../../data/clean/ttm_weighted_scores.csv", row.names = FALSE)
```




