---
title: "Travel Time Matrix to Maps"
output: html_notebook
---

## Notebook Purpose

This notebook serves to summarize the entire visualization process going from 
the travel time matrix to the visualizations. That involves the following 
sections:

1) Travel Time Matrix Wrangling
2) Score Computation
3) Isochrone Computation
4) Dataset Wrangling Part II (NA Insertion)
5) Interactive Visualization
6) Map HTML Exports

## 0) Useful Libraries

```{r message=FALSE, warning=FALSE, include=TRUE}
# import custom scoring, cleaning, and visualization functions
source('functions_eff.R')

# wrangling/convenience
library(tidyverse)
library(glue)
library(stringr)
library(sf)
library(data.table)
library(bit64)

# visualization
library(leaflet)
library(mapview); mapviewOptions(platform = 'leafgl')

# For pretty knitting
library(lemon)
knit_print.data.frame <- lemon_print
knit_print.tbl <- lemon_print
knit_print.summary <- lemon_print
```




## 1) Initial data wrangling
```{r}

# Function to calculate efficiency withing 'X' degrees of each block

#import scores
scores_long <- read.csv('../../data/efficiency/initial_scores.csv', stringsAsFactors = FALSE)
origins <- fread(file.path("../../data/clean", "vancouver_db.csv"), stringsAsFactors = FALSE)[, .(id, pop, lat, lon)]
traffic_data <- read.csv('../../data/raw/BC_trafic_data_2015_summary.csv', stringsAsFactors = FALSE)[c(9,10,16:37)]
db_fac_dense <- read.csv('../../data/efficiency/ttm_eff.csv', stringsAsFactors = FALSE)

origins$pop <- str_replace_all(origins$pop, ',', '')

# change col types

db_fac_dense$fromId <- as.factor(db_fac_dense$fromId)
scores_long$fromId <- as.factor(scores_long$fromId)
origins$pop <- as.numeric(origins$pop)  
origins$id <- as.factor(origins$id)  
n_origins <- nrow(origins)
paste('Origin Rows: ', n_origins)
```

## 2) Calculation of the running efficiency score
## Calculate using existing proximity scores and traffic vehicle count
```{r}
# Get mean accessibility score for each block
scores_long_pos <- filter(scores_long,stringr::str_detect(nearest_n, '1') & stringr::str_detect(weight, 'no'))[,c(1,3)] %>% group_by(fromId) %>% summarise(mean_score = mean(score))


# population
scores <- left_join(scores_long_pos, origins[, c('id', 'pop')], by=c("fromId" = "id"), keep = FALSE)

# amenity density
db_fac_dense2 <- db_fac_dense %>% group_by(fromId) %>% summarize(amn_dens = as.double(n()))

scores <- left_join(scores, db_fac_dense2, by=c('fromId' = "fromId"), keep = FALSE)

# Normalize the population of each block
scores[is.na(scores$amn_dens), c("amn_dens")] <- 0
scores <- normalize_df(scores)

scores
```



```{r}

# Get most recent mean vehicle count of all instruments within 5 km of each data block
traffic_data$TrafficCount <- apply(traffic_data, 1, getAll_Data)
traffic_data_clean <- traffic_data[, c(1,2,25)]

scores_pos <- left_join(scores, origins[, c('id', 'lat', 'lon')], by=c("fromId" = "id"), keep = FALSE)


db_trafic <- function(row){
  mean(filter(traffic_data_clean, 
              traffic_data_clean$LATITUDE <= (as.numeric(row["lat"])+0.0675) & 
              traffic_data_clean$LATITUDE >= (as.numeric(row["lat"])-0.0675) & 
              traffic_data_clean$LONGITUDE <= (as.numeric(row["lon"])+0.0675) & 
              traffic_data_clean$LONGITUDE >= (as.numeric(row["lon"])-0.0675))$TrafficCount, na.rm = TRUE) 
}

scores_pos$trafficScore <- apply(scores_pos,1,db_trafic)

scores_pos$trafficScore[is.na(scores_pos$trafficScore)] <- mean(scores_pos$trafficScore, na.rm = TRUE)
scores_pos$trafficScore <- normalize_vec(scores_pos$trafficScore)


# copy score frame but with percentiles instead of raw scores
scores_pos_percentiles <- scores_pos

# convert need factors to percentile to make them comparable
perc_traffic <- ecdf(scores_pos$trafficScore)
perc_pop <- ecdf(scores_pos$pop) 
perc_amn <- ecdf(scores_pos$amn_dens)

scores_pos_percentiles$trafficScore <- perc_traffic(scores_pos_percentiles$trafficScore)
scores_pos_percentiles$pop <- perc_pop(scores_pos_percentiles$pop)
scores_pos_percentiles$amn_dens <- perc_amn(scores_pos_percentiles$amn_dens)

# compute need
scores_pos_percentiles$need <- (scores_pos_percentiles$trafficScore + scores_pos_percentiles$pop + scores_pos_percentiles$amn_dens)/3


# optionally convert accs and need score to percentile
perc_accs <- ecdf(scores_pos$mean_score) 
perc_need <- ecdf(scores_pos_percentiles$need)

scores_pos_percentiles$mean_score <- perc_accs(scores_pos_percentiles$mean_score)
scores_pos_percentiles$need <- perc_need(scores_pos_percentiles$need)


## Calculate efficiency score

# log distribution adjustment
#scores_pos_percentiles$eff <- normalize_vec(log(scores_pos_percentiles$mean_score)) - scores_pos_percentiles$need

# percentile distribution adjustment
scores_pos_percentiles$eff <- normalize_vec(scores_pos_percentiles$mean_score - scores_pos_percentiles$need, x = -1, y = 1)


efficiency_frame <- scores_pos_percentiles[, -c(5,6)]
efficiency_frame
```






## 3) Dataset Wrangling Part II (NA Insertion)

Each origin(fromId) should have x different scores where x is defined by:

x = 4 amenity options = 4

In addition to filling the empty NA rows for included IDs, there are also IDs
that need to be re-added as not a single time was computed for these IDs.


```{r}
paste0('EFFICIENCY FRAME')

# HOW MANY ROWS TO ADD IN EFFICIENCY FRAME?
# existing IDs that weren't included in ttm
missing_blocks <- array(setdiff(origins$id, efficiency_frame$fromId))
total_expected <- nrow(efficiency_frame) + length(missing_blocks) # only 4 values per origin
paste(glue('{nrow(efficiency_frame)} of {total_expected} rows filled ({round((nrow(efficiency_frame) / total_expected)*100, 2)}%)'))
paste(length(missing_blocks), 'to add')
```

```{r}

# add missing NAs to efficiency frame
all_efficiency_frame <- NA_table_filler_eff(efficiency_frame, custom_idx = missing_blocks)

# check efficiency
# there should be 1 count per fromId
eff_id_counts <- all_efficiency_frame %>% group_by(fromId) %>% summarise(n = n())
unique(eff_id_counts$n)

```


```{r}

## Export checkpoint
write.csv(all_efficiency_frame, '../../data/efficiency/efficiency_frame.csv', row.names = FALSE)

```


### Import the dissemination block shape file
```{r}
canada_shape <- st_read("../../data/census2016_DBS_shp/DB_Van_CMA/DB_Van_CMA.shp", stringsAsFactors = FALSE)

# select a greater metropolitan area
metropolitan_area <- "Vancouver"

# filter columns and rows
vancouver_shape <- data.frame(canada_shape[which(canada_shape$CMANAME == metropolitan_area), c(1, 28)])

# id to factor
vancouver_shape$DBUID <- as.factor(vancouver_shape$DBUID)

paste('Rows = ', nrow(vancouver_shape))
head(vancouver_shape)
```


## 4) Interactive Visualization
```{r}
# join factor and geometry data 
eff_viz_frame <- left_join(vancouver_shape, all_efficiency_frame, by = c('DBUID' = 'fromId'))

# convert back to sf object
eff_viz_frame_sf <- st_as_sf(eff_viz_frame)

# convert to st object
eff_viz_frame_st <- st_transform(eff_viz_frame_sf, crs = 4326)

```



## 5) Map HTML Exports

```{r}
source('functions_eff.R')

map_maker_efficiency_num(data = eff_viz_frame_st,
                      output_dir = '../../data/html_maps/efficiency_maps')

map_maker_efficiency_quant(data = eff_viz_frame_st,
                      output_dir = '../../data/html_maps/efficiency_maps')
```


```{r}
plot(density(normalize_vec(efficiency_frame$mean_score)))

plot(density(efficiency_frame$need))

plot(density(efficiency_frame$eff))


```





























