---
title: "Travel Time Matrix to Maps"
output: html_notebook
---

## Notebook Purpose

This notebook serves to summarize the entire visualization process going from 
the travel time matrix to the visualizations. That involves the following 
sections:

1) Travel Time Matrix Wrangling
2) Score Computation
3) Isochrone Computation
4) Dataset Wrangling Part II (NA Insertion)
5) Interactive Visualization
6) Map HTML Exports

## 0) Useful Libraries

```{r message=FALSE, warning=FALSE, include=TRUE}

# import custom  functions
source('../../0 - custom_functions/functions.R')

# wrangling/convenience
library(tidyverse)
library(glue)
library(stringr)
library(sf)
library(data.table)
library(bit64)

# visualization
library(leaflet)
library(mapview); mapviewOptions(platform = 'leafgl')

# For pretty knitting
library(lemon)
knit_print.data.frame <- lemon_print
knit_print.tbl <- lemon_print
knit_print.summary <- lemon_print

datapath <- "../../../data/"

```




## 1) Initial data import
```{r}


scores <- read.csv(paste0(datapath, '3_computed/accessibility_measures/scores_frame.csv'), stringsAsFactors = FALSE)

origins <-  fread(file.path("../../../data/2_clean/vancouver_db.csv"))


traffic_data <- read.csv(paste0(datapath, '1_raw/traffic_data/BC_traffic_data_2015_summary.csv'), stringsAsFactors = FALSE)[c(9,10,16:37)]

amenity_density <- read.csv('../../../data/3_computed/amenity_density.csv', stringsAsFactors = FALSE)


# change col types
scores[c(1,2,4,5)] <- lapply(scores[c(1,2,4,5)], as.factor)
amenity_density$fromId <- as.factor(amenity_density$fromId)
origins$id <- as.factor(origins$id)

```

## Combing efficiency factors (population, amenity density, and traffic data)

```{r message=FALSE, warning=FALSE}

# Get mean accessibility score for each block
eff_frame <- filter(scores, weight=='no' & nearest_n ==1,)[c(1,3)] %>%
                          #str_detect(nearest_n, '1') & str_detect(weight, 'no'))[,c(1,3)] %>%
                   group_by(fromId) %>%
                   summarise(mean_score = mean(score, na.rm = TRUE))

# add coordinates
eff_frame <- left_join(eff_frame, origins[, c('id', 'lat', 'lon')], by=c("fromId" = "id"), keep = FALSE)

# add population
pops <- scores %>% group_by(fromId, pop) %>% summarize()
eff_frame <- left_join(eff_frame, pops, by=c("fromId" = "fromId"), keep = FALSE)

# add amenity density (frequency in this case)
eff_frame <- left_join(eff_frame, amenity_density, by=c('fromId' = "fromId"), keep = FALSE)
eff_frame <- eff_frame %>% rename(amn_density = n_amenities)

# assign NAs zero value
eff_frame[is.na(eff_frame$amn_density), c("amn_density")] <- 0

# Normalize the population of each block
eff_frame$pop <- normalize_vec(eff_frame$pop)
eff_frame$amn_density <- normalize_vec(eff_frame$amn_density)

eff_frame
```


*Here we combine the traffic data*

The 0.065 degrees needs to be checked for its actual distance

```{r}

# Get most recent mean vehicle count of all instruments within 5 km of each data block
traffic_data$TrafficCount <- apply(traffic_data, 1, getAll_Data)
traffic_data_clean <- traffic_data[, c(1,2,25)]


db_trafic <- function(row){
  mean(filter(traffic_data_clean, 
              traffic_data_clean$LATITUDE <= (as.numeric(row["lat"])+0.065) & 
              traffic_data_clean$LATITUDE >= (as.numeric(row["lat"])-0.065) & 
              traffic_data_clean$LONGITUDE <= (as.numeric(row["lon"])+0.065) & 
              traffic_data_clean$LONGITUDE >= (as.numeric(row["lon"])-0.065))$TrafficCount, na.rm = TRUE) 
}

eff_frame$trafficScore <- apply(eff_frame, 1, db_trafic)

# replace NA values with 20th percentile traffic 
eff_frame$trafficScore[is.na(eff_frame$trafficScore)] <- quantile(ecdf(eff_frame$trafficScore), 0.2)
eff_frame$trafficScore <- normalize_vec(eff_frame$trafficScore)
```

```{r}

# copy score frame to work with percentiles instead of raw scores
eff_frame_percentiles <- eff_frame
library(fmsb)

# convert the "needs" factors to percentile to make them comparable
eff_frame_percentiles$trafficScore <- percentile(eff_frame$trafficScore)/100
eff_frame_percentiles$pop <- percentile(eff_frame$pop)/100
eff_frame_percentiles$amn_density <- percentile(eff_frame$amn_density)/100

# compute needs factor
eff_frame_percentiles$need <- (eff_frame_percentiles$trafficScore + 
                               eff_frame_percentiles$pop + 
                               eff_frame_percentiles$amn_density)/3


# convert accessibility scores and needs score to percentile to make them comparable as well
# first we need to assign rows with NA as 0 since they have no access
# or we can remove them entirely
eff_frame_percentiles$mean_score[is.na(eff_frame_percentiles$mean_score)] <- 0.01

eff_frame_percentiles$mean_score <- percentile(eff_frame_percentiles$mean_score)/100
eff_frame_percentiles$need <- percentile(eff_frame_percentiles$need)/100


## Calculate efficiency score

# log distribution adjustment - dont use this
#scores_pos_percentiles$eff <- normalize_vec(log(scores_pos_percentiles$mean_score)) - scores_pos_percentiles$need

# percentile distribution adjustment
eff_frame_percentiles$efficiency <- normalize_vec(
  eff_frame_percentiles$mean_score - eff_frame_percentiles$need,
  x = -1, y = 1)

efficiency_frame <- eff_frame_percentiles
efficiency_frame

```



```{r}

## Export checkpoint
write.csv(efficiency_frame, '../../../data/3_computed/transit_efficiency/efficiency_frame.csv', row.names = FALSE)

```

## 3) SECTION NO LONGER NEEDED: Dataset Wrangling Part II (NA Insertion)

Each origin(fromId) should have x different scores where x is defined by:

x = 4 amenity options = 4

In addition to filling the empty NA rows for included IDs, there are also IDs
that need to be re-added as not a single time was computed for these IDs.


```{r}
paste0('EFFICIENCY FRAME')

# HOW MANY ROWS TO ADD IN EFFICIENCY FRAME?
# existing IDs that weren't included in ttm
missing_blocks <- array(setdiff(origins$id, efficiency_frame$fromId))
total_expected <- nrow(efficiency_frame) + length(missing_blocks) # only 4 values per origin

paste(glue('{nrow(efficiency_frame)} of {total_expected} rows filled ({round((nrow(efficiency_frame) / total_expected)*100, 2)}%)'))
paste(length(missing_blocks), 'to add')
```

```{r}

# add missing NAs to efficiency frame
all_efficiency_frame <- NA_table_filler_eff(efficiency_frame, custom_idx = missing_blocks)

# check efficiency: there should be 1 count per fromId
eff_id_counts <- all_efficiency_frame %>% group_by(fromId) %>% summarise(n = n())
unique(eff_id_counts$n)

```




### Import the dissemination block shape file
```{r}
canada_shape <- st_read("../../data/census2016_DBS_shp/DB_Van_CMA/DB_Van_CMA.shp", stringsAsFactors = FALSE)

# select a greater metropolitan area
metropolitan_area <- "Vancouver"

# filter columns and rows
vancouver_shape <- data.frame(canada_shape[which(canada_shape$CMANAME == metropolitan_area), c(1, 28)])

# id to factor
vancouver_shape$DBUID <- as.factor(vancouver_shape$DBUID)

paste('Rows = ', nrow(vancouver_shape))
head(vancouver_shape)
```


## 4) Interactive Visualization
```{r}
# join factor and geometry data 
eff_viz_frame <- left_join(vancouver_shape, all_efficiency_frame, by = c('DBUID' = 'fromId'))

# convert back to sf object
eff_viz_frame_sf <- st_as_sf(eff_viz_frame)

# convert to st object
eff_viz_frame_st <- st_transform(eff_viz_frame_sf, crs = 4326)

```



## 5) Map HTML Exports

```{r}
source('functions_eff.R')

map_maker_efficiency_num(data = eff_viz_frame_st,
                      output_dir = '../../data/html_maps/efficiency_maps')

map_maker_efficiency_quant(data = eff_viz_frame_st,
                      output_dir = '../../data/html_maps/efficiency_maps')
```


```{r}
plot(density(normalize_vec(efficiency_frame$mean_score)))

plot(density(efficiency_frame$need))

plot(density(efficiency_frame$eff))


```





























