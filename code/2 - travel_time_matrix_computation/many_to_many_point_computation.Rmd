---
title: "Many-to-Many Point Computation Script"
author: "Luka Vukovic"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

### Loading libraries

```{r include=TRUE, message=FALSE, warning=FALSE}

# Main
library(r5r)
library(sf)
library(data.table)

if (Sys.getenv("JAVA_HOME")!="") { Sys.setenv(JAVA_HOME="") }
library(rJava)

# Convenience
library(tidyverse)
library(glue)
library(rlist)

# For pretty knitting
library(lemon)
knit_print.data.frame <- lemon_print
knit_print.tbl <- lemon_print
knit_print.summary <- lemon_print

path_data <- "../../data/2_clean/"

```


### 1. Setting Up the Network Graph

- To compute transit accessibility measures we must first build the
transit network in R using the r5r library.
- If the vancouver_canada.osm.pbf file needs to be converted to an .osm, one can use a binary osm converter available at: https://wiki.openstreetmap.org/wiki/Osmconvert#Binaries

*Note: Graph network building may take up to a few minutes, especially for large cities*

```{r message=FALSE, warning=FALSE, eval=FALSE}

# allocate an appropriate RAM to Java
options(java.parameters = "-Xmx6g")

# build the transit network
# OSM file and GTFS.zip are in the same directory in this case
r5r_core <- setup_r5(data_path = getwd(), verbose = FALSE)

```


### 2. Loading the Origins and Destinations

- Origins correspond to the centroid coordinates of Canadian Census 2016
dissemination blocks across Canada. This was filtered by metropolitan area (MA)
to keep only Greater Vancouver city blocks.
- Destinations correspond to the amenity coordinates of the OCDAF database.

```{r include=TRUE, message=FALSE, warning=FALSE}

## ORIGINS

# dissemination blocks via fread, a faster way to import than read_csv
origins <- fread(file.path(paste0(path_data, "vancouver_db.csv")))

# remove population column
origins <- origins[, -2]

# conver id numeric to char
origins$id <- as.character(origins$id)

paste('Origins: ', nrow(origins))
head(origins)
```

```{r include=TRUE, message=FALSE, warning=FALSE}

## DESTINATIONS

# cultural/Art facilities
destinations <- fread(file.path(paste0(path_data, "vancouver_amenities.csv")))

# see summary counts of each amenity
destinations %>% group_by(type) %>% summarise(count = n()) %>% arrange(desc(count))

# filter amenity types to keep the amenities of interest
target_amenities <- c('gallery',
                      'museum',
                      'library or archives',
                      'theatre/performance and concert hall')

destinations <- destinations %>% filter(type %in% target_amenities)

# keep id, lat, and lon columns
destinations <- destinations[, 1:3]

destinations$lat <-  as.numeric(destinations$lat) # char to numeric
destinations$lon <-  as.numeric(destinations$lon) # char to numeric
destinations$id <- as.character(destinations$id)  # numeric to char

paste('Destinations with NA: ', nrow(destinations))
destinations <- destinations[complete.cases(destinations)] # remove NA rows
paste('Destinations clean: ', nrow(destinations))

# peek
head(destinations)

```


### 3. Setting Travel Constraints

- For each computed travel time via transit, we set constraints to model more 
realistic uses of transit networks.
- For example, most people wont take the bus if it take longer than 2 hours,
or if they need to walk more than a kilometer on the trip, or if they need to 
take more than 3 transfers and so on.

```{r include=TRUE, message=FALSE, warning=FALSE}

# Non-transit modes: WALK, BICYCLE, CAR, BICYCLE_RENT, CAR_PARK
# Transit modes: TRAM, SUBWAY, RAIL, BUS, FERRY, CABLE_CAR, GONDOLA, FUNICULAR
# default walk speed = 3.6 km/h

mode <- c('WALK', 'TRANSIT')
max_walk_dist <- 1000 # 1 km
max_trip_duration <- 120 # 2 hours
max_rides <- 3 # max transfers

```


### 4. Computing the Travel Time Matrix

- To measure a city blocks accessibility to all amenities within the
constraints,we need to consider averaging travel times across both a weekly
bus schedule, a saturday bus schedule, and a sunday bus schedule.
- Furthermore, we also want to average transit times across the time of day
so we compute a travel time every hour from 7am to 7pm with a departure window
of 30 minutes.

*Note: this operation only functions on a single cpu core so the*
*cell can take a few hours to execute. Please be patient or consider using*
*less origins or destinations or time points.*

```{r message=FALSE, warning=FALSE, include=TRUE, eval=FALSE}

# collect each travel time matrix from every hour
all_ttms <- list()
all_hours <- c()

# time range from 07:00 to 19:00
list1 <- 7:19


for (day in 14:16) {  # May 14=Fri, 15=Sat, 16=Sun
  
  hourly <- c()
  t1 <- c()
  
  for (time in 7:19) {        # 7 to 19 hours
    
    departure_datetime <- as.POSIXct(glue("{day}-05-2021 {time}:00:00"), format="%d-%m-%Y %H:%M:%S")
    
    ttm <- travel_time_matrix(r5r_core = r5r_core,
                          origins = origins,
                          destinations = destinations,
                          departure_datetime = departure_datetime,
                          time_window = 30,
                          
                          # constrains
                          mode = mode,
                          max_walk_dist = max_walk_dist,
                          max_trip_duration = max_trip_duration,
                          max_rides = max_rides,
                          verbose = FALSE)
    
    # collect time for each trip based on the number of travel time (row) computed by r5r
    hourly <- append(hourly, nrow(ttm))
    
    # do NOT use rbind(all_ttm, ttm), it is insultingly slow
    # append to a list then use rbindlist
    all_ttms <-  list.append(all_ttms, ttm)
    
    print(glue('Progress: {round(((day-14)*12 + time-6)/37*100, 1)}%'))
  }
  
# To add time column for each day (hourly)

  for (i in 1:length(hourly)) {  # number of travel time computed per hour depends on r5r
    
    t <- rep(list1[i], times = hourly[i])
    t1 <- append(t1, t)
  }
  
  t0 <- t1[1:sum(hourly[1:3])]
  
  t0 <- paste0("0", t0)
  t1[1:sum(hourly[1:3])] <- t0
  
  t2 <- mapply(function(x,y) paste0(rep(x,y), collapse = ""), 0, 4 - nchar(t1))
  t1 <- paste0(t1, t2)
  
  # convert to time format
  t1 <- format(strptime(t1, format="%H%M"), format = "%H:%M") 
  
  all_hours <-  append(all_hours, t1)
  
}

# Fast way to bind all data.frames

TTM <- rbindlist(all_ttms)

# Add time column to the travel time matrix
all_hours_ttms <- cbind(all_hours, TTM)

print('COMPLETED')

summary(all_hours_ttms)
```


### 5. Aggregating Travel Time Matrix 

- We are interested in the average transit time across all 36 departure times
so we aggregate on the origin and destination (ie. on every unique trip).
- For comparing transit accessibility between different days and time see the
**ttm_time_unaggregated** folder.

```{r message=FALSE, warning=FALSE, include=TRUE, eval=FALSE}

TTM_agg <- TTM %>%
           group_by(fromId, toId) %>% 
           summarise(
             avg_time = mean(travel_time), # average time
             sd_time = sd(travel_time)     # standard deviation of avg time
           )

```


```{r message=FALSE, warning=FALSE, include=TRUE}

# for knitting we just skip the TTM computation and aggregation
TTM_agg <- read.table(gzfile(paste0("../../data/3_computed/main_travel_time_matrix--time_aggregated.csv.gz"),
                         "main_travel_time_matrix--time_aggregated.csv"), # file within zip
                  header=T, quote="\"", sep=",") # format into a table

paste('Note this is the cleaned version that was imported to skip the computation cell during knitting.')

summary(TTM_agg)

```


### 6. Fixing Odd Values in sd_time

**NA standard deviations**
- Since some origins may only have only a single trip, their standard deviation
will be undefined as you need at least 2 values for standard deviation.
- We simply replace them with the median trip standard deviation.
- This is not the worst assumption to make since the bus still probably
comes on a regular schedule with a standard uncertainty to most stops.

**Zero standard deviations**
- We can also imagine many trips have no standard deviation as their travel
time is always identical across multiple days or hours.
- To avoid any issues with zero division or massive numerators from small
division (in case we use a different scoring formula), we can replace these
cases with 1 minute since 1 minute is the smallest realistic standard
deviation in any travel time.

```{r message=FALSE, warning=FALSE, include=TRUE}

# replace NAs
median_sd <- median(TTM_agg$sd_time, na.rm=TRUE)
TTM_agg <- TTM_agg %>% replace_na(list('sd_time' = median_sd))

# replace < 1 with 1 to avoid large or infinity computations later on
TTM_agg$sd_time[(TTM_agg$sd_time < 1)] <-  1

paste('Clean TTM Aggregation:')
summary(TTM_agg)

```


### 7. Exporting the Travel Time Matrix for Further Work

- This summarizes our primary method for the first step in efficient
transit network analysis: **Obtaining Realistic Travel Time Data**

```{r message=FALSE, warning=FALSE, include=TRUE}

# for compressed output - otherwise file is too big to be pushed to github
library('readr')

# Export the travel time matrix with time windows (for nearest isochrone)

write_csv(all_hours_ttms, "../../data/3_computed/main_travel_time_matrix_with_timeWindow.csv.gz")

# Export the travel time matrix with aggregated avg and sd (for score)

write_csv(TTM_agg, "../../data/3_computed/main_travel_time_matrix--time_aggregated.csv.gz")

# you may use this function to perform imports on compressed files:
# data_here <- read.table(
#                 unz("path/to/data/3_computed/main_travel_time_matrix--time_aggregated.zip",
#                     "main_travel_time_matrix--time_aggregated.csv"), # file within zip
#                 header=T, quote="\"", sep=",") # format into a table

```

